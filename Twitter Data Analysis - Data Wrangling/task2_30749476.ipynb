{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name: Manmeet Singh**<br>\n",
    "**Student Id: 30749476**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**<br>\n",
    "Write Python code to preprocess a set of tweets and convert them into numerical representations (which are suitable for input into recommender-systems/ information-retrieval algorithms).<br>\n",
    "1. Generate the corpus vocabulary with the same structure as sample_vocab.txt. Vocabulary must be sorted alphabetically.<br>\n",
    "2. For each day (i.e., sheet in your excel file), calculate the top 100 frequent unigram and top-100 frequent bigrams. If you have less than 100 bigrams for a particular day, just include the top-n bigrams for that day (n<100).<br>\n",
    "3. Generate the sparse representation (i.e., doc-term matrix) of the excel file according.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import langid\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Excel file and Stopword file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the excel file\n",
    "sheetName = pd.ExcelFile('30749476.xlsx')\n",
    "\n",
    "# Open stopwords file and create a list of stopwords\n",
    "with open('stopwords_en.txt', 'r') as stop:\n",
    "    stopWords = stop.read().split()\n",
    "\n",
    "# initializing an empty data frame\n",
    "finalData = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to perform.<br>\n",
    "1. Using the “langid” package, only keeps the tweets that are in English language.<br>\n",
    "2. The word tokenization must use the following regular expression, \"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\".<br>\n",
    "3. The context-independent and context-dependent (with the threshold set to more than 60 days ) stop words must be removed from the vocab. The provided context-independent stop words list (i.e, stopwords_en.txt ) must be used.<br>\n",
    "4. Tokens should be stemmed using the Porter stemmer.<br>\n",
    "5. Rare tokens (with the threshold set to less than 5 days ) must be removed from the vocab.<br>\n",
    "6. Creating sparse matrix using countvectorizer.<br>\n",
    "7. Tokens with the length less than 3 should be removed from the vocab.<br>\n",
    "8. First 200 meaningful bigrams (i.e., collocations) must be included in the vocab using PMI measure.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology<br>\n",
    "\n",
    "1. Initialize empty list called finalList to store all the unique words after removing stop words from tweets.<br>\n",
    "2. Initiate for loop which read each sheet one by one and perform following steps on data of each sheet:<br>\n",
    "    a. Read data form sheet in a dataframe.<br>\n",
    "    b. Drop columns and rows having Nan values. Drop only for those which have all the columsn and rows as Nan.<br>\n",
    "    c. Drop first row and rename the column as text, id, and created_at.<br>\n",
    "    d. Reset the index.<br>\n",
    "    e. Assign column created_at value of date itself, as it contains date in UTC format.<br>\n",
    "    f. Remove all the data from text column which is not a string or tweet.<br>\n",
    "    g. Create new column lang to classify the language of tweet and then filter out non-english tweets.<br>\n",
    "    h. Initialize tokenizer **(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")**.<br>\n",
    "    i. Create column tokens and create tokens of tweets using above tokenizer.<br>\n",
    "    j. Create contextIndependent column and remove all the stop words and tokens having length less than 3.<br>\n",
    "    k. Unique list of words from contextIndependent. Will be used to remove context Dependent words.<br>\n",
    "    l. Append the dataframe and add all the data from each sheet to dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty list called finalList to store all the unique words after removing stop words from tweets\n",
    "finalList = []\n",
    "for sheet in sheetName.sheet_names:\n",
    "\n",
    "    file = pd.read_excel(\"30749476.xlsx\", sheet_name=sheet)\n",
    "      # drop all Nan columns and rows\n",
    "    file.dropna(axis =1, how='all', inplace=True)\n",
    "    file.dropna(how='all', inplace=True)\n",
    "    # drop first row\n",
    "    file.drop([file.index[0]], inplace=True)\n",
    "    # rename columns names\n",
    "    file.rename(columns={file.columns[0]: \"text\", file.columns[1]: \"id\", file.columns[2]: \"created_at\"}, inplace=True)\n",
    "    # reset index of dataframe\n",
    "    file.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # file = file[file[\"text\"].apply(lambda x: len(str(x))>1)]\n",
    "     # assign column created_at value of date as it contains date in UTC format\n",
    "    file['created_at'] = sheet\n",
    "     # remove rows which are not string or tweet.\n",
    "    file = file[file[\"text\"].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "    file[\"lang\"] = [langid.classify(i)[0] for i in file[\"text\"]]\n",
    "\n",
    "    # filtering english tweets\n",
    "    file = file[file[\"lang\"] == 'en']\n",
    "    \n",
    "    # initiate tokenizer\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "    \n",
    "    # creating tokens from the tweets.\n",
    "    file[\"tokens\"] = file[\"text\"].apply(lambda x: tokenizer.tokenize(x))\n",
    "    \n",
    "    # removing stop words\n",
    "    file[\"contextIndependent\"] = file[\"tokens\"].apply(lambda x: [i for i in x if i not in stopWords])\n",
    "    \n",
    "    # removing tokens with length less than 3\n",
    "    file[\"contextIndependent\"] = file[\"contextIndependent\"].apply(lambda x: [i for i in x if len(i) > 2])\n",
    "    \n",
    "#     # adding all the tokens to allList\n",
    "    allList = []\n",
    "    for i in file[\"contextIndependent\"]:\n",
    "        allList.extend(i)\n",
    "    # creating a unique list to remove threshold stop words\n",
    "    final = list(set(allList))\n",
    "    \n",
    "    # create unique set of words\n",
    "    finalList.extend(final) \n",
    "    \n",
    "    \n",
    "#     allList.clear()\n",
    "    \n",
    "\n",
    "    finalData = finalData.append(file, ignore_index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Dependent and Threshold words.<br>\n",
    "\n",
    "1. Create empty dictionary stopThresDict and take count of unique words created in above section (finalList).<br>\n",
    "2. Create empty lisy stopThresWords and add all the words whose count is less than 60 and equal and more than 5.<br>\n",
    "3. These words are context dependent words based on threshold more than 60 days and less than 5 days.<br>\n",
    "4. Create contextDependent column in finalData and remove all the threshold words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty dictionary to take count of words\n",
    "stopThresDict = {}\n",
    "# take count of all unique words from context independent column.\n",
    "for i in finalList:\n",
    "        if i in stopThresDict:\n",
    "            stopThresDict[i]+=1\n",
    "        else:\n",
    "            stopThresDict[i] = 1\n",
    "# create empty list\n",
    "stopThresWords = []\n",
    "# store all words based on threshold limit of more than 60 days and less than 5 days.\n",
    "for key,value in stopThresDict.items():\n",
    "    \n",
    "    if value < 5:\n",
    "        stopThresWords.append(key)\n",
    "    elif value > 60:\n",
    "        stopThresWords.append(key)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# contextDependent column after removing threshold words.\n",
    "finalData[\"contextDependent\"] = finalData[\"contextIndependent\"].apply(lambda x: [word for word in x if word not in stopThresWords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porter Stemming<br>\n",
    "\n",
    "Stemm all the words in contextDependent based on porter stemming and store in column stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "finalData[\"stemmer\"] = finalData[\"contextDependent\"].apply(lambda x: [ps.stem(word) for word in x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uni gram Dictionary<br>\n",
    "\n",
    "1. Creating uni gram from stemmer column.<br>\n",
    "2. Loop over finalData dataframe date wise and take count of all the words and store in uniDict and word:count as key:value pair.<br>\n",
    "3. Add word:count pair in list as tuple.<br>\n",
    "4. Sort all the key value pair based on count of words and store them according to dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniDict = {} # create empty dictionary\n",
    "# loop over the data frame based on dates\n",
    "for date in sheetName.sheet_names:\n",
    "    uni = {}\n",
    "    uniList = []\n",
    "    # filter data based on date to avoid duplicate entries\n",
    "    df = finalData[finalData[\"created_at\"] == date]\n",
    "    \n",
    "    # take count of all the words in column stemmer\n",
    "    for text in df[\"stemmer\"]:\n",
    "        for words in text:\n",
    "            if words in uni:\n",
    "                uni[words]+=1\n",
    "            else:\n",
    "                uni[words] = 1\n",
    "    # add key value pair of word and count to list as tuple.\n",
    "    for key, values in uni.items():\n",
    "        uniList.append((key, values))\n",
    "        \n",
    "    # sort the date in uniList as per the count\n",
    "    uniList.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    uniDict[date] = uniList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Bi grams (top 200 based on PMI measure)<br>\n",
    "\n",
    "1. Define function top200Bigram and set PMI measure = 200.<br>\n",
    "2. Creating bi gram from tokens column.<br>\n",
    "3. Loop over finalData dataframe date wise and pass all the tokens to top200Bigram fuction to generate tuple of bi grams.<br>\n",
    "4. Join the biagrams with \"_\" and add to dictionary as value and date as key.<br>\n",
    "5. Add all the bi grams to uniqueBi list and create a unique list of bigrams.<br>\n",
    "6. Take count of all the bi grams and add to list as pair of word:count as a tuple.<br>\n",
    "5. Sort all the key value pair based on count of words and store them according to dates in biGramDict dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code was taken and refered from lab excercise\n",
    "\n",
    "# define fuctioin top200Bigram and assign pmi_measure=200 as we need to extract 200 meaning full bigrams\n",
    "def top200Bigram(tokenList, pmi_measure = 200):\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(tokenList)\n",
    "    bigram_finder.apply_freq_filter(20)\n",
    "    topNBigrams = bigram_finder.nbest(bigram_measures.pmi, pmi_measure)\n",
    "    return topNBigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bi Gram\n",
    "biDict = {}\n",
    "# emplty list for bi grams\n",
    "extened_biGram_List = []\n",
    "for date in sheetName.sheet_names:\n",
    "    # empty list\n",
    "    TList = []\n",
    "    # filter dataframe as per the date\n",
    "    biGram = finalData[finalData[\"created_at\"] == date]\n",
    "    \n",
    "    # loop over all the tokens and add to TList\n",
    "    for token in biGram[\"tokens\"]:\n",
    "        TList.extend(token)\n",
    "    \n",
    "    # pass TList as parameter to top200Bigram function\n",
    "    top_200 = top200Bigram(TList)\n",
    "    # join bi grams with \"_\"\n",
    "    biDict[date] = [\"_\".join(x) for x in top_200] \n",
    "    # add bi grams to list\n",
    "    extened_biGram_List.extend(biDict[date])\n",
    "# create unique list of bi grams\n",
    "uniqueBi = list(set(extened_biGram_List))\n",
    "\n",
    "# empty bi gram dictionary\n",
    "biGramDict = {}\n",
    "# extened_biGram_List = []\n",
    "for keys, value in biDict.items():\n",
    "    TempDict = {}\n",
    "    biGramList = []\n",
    "    # take count of bi grams\n",
    "    for word in value:\n",
    "        if word in TempDict:\n",
    "            TempDict[word]+=1\n",
    "        else:\n",
    "            TempDict[word]=1\n",
    "    # save word count as tuple in list\n",
    "    for word, count in TempDict.items():\n",
    "        biGramList.append((word, count))\n",
    "    # sort based on count\n",
    "    biGramList.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    # update date key value as sorted list\n",
    "    biGramDict[keys] = biGramList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocabulary<br>\n",
    "\n",
    "1. Take set of words in stemmer column of dataframe.<br>\n",
    "2. Add add uniqueBi to the uniqueVocab list.<br>\n",
    "3. Sort the list alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabList = []\n",
    "# add all words of stemmer to lsit\n",
    "for tokens in finalData[\"stemmer\"]:\n",
    "    vocabList.extend(tokens)\n",
    "# take set of vocabList to get unique vocab\n",
    "uniqueVocab = list(set(vocabList))\n",
    "# add bi gram to this uniqueVocab\n",
    "uniqueVocab.extend(uniqueBi)\n",
    "# sort the list\n",
    "uniqueVocab.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating final vocabulary \n",
    "Vocab = {}\n",
    "# initiate serial number\n",
    "i = 0\n",
    "for words in uniqueVocab:\n",
    "    Vocab[words] = i # word and serial number in alphabetical order \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine values of uni and bi gram based on date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Uni and Bi gram dictionaries together\n",
    "combined_Grams = {}\n",
    "# add values of bi gram to uni gram based on date as key.\n",
    "for date in sheetName.sheet_names:\n",
    "    combined_Grams[date] = uniDict[date]+biGramDict[date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vector (doc-term matrix)<br>\n",
    "\n",
    "1. Loop over combined_Grams dictionary.<br>\n",
    "2. Fetch serial number from Vocab dictionary and count of term from combined_Grams.<br>\n",
    "3. Add the doc-term result to list for each date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty countVec list\n",
    "countVec = []\n",
    "\n",
    "for key,value in combined_Grams.items():\n",
    "    dataList = []\n",
    "    dataList.append(key) # append dataList with date\n",
    "    for words in value:\n",
    "        # create doc:term list\n",
    "        x = str(Vocab[words[0]])+\":\"+str(words[1])\n",
    "        # append dataList with doc:term\n",
    "        dataList.append(x)\n",
    "    # append each list for respective date to countVec\n",
    "    countVec.append(dataList)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Vocab, countVec, uni and bi gram to text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('30749476_100uni.txt','w+', encoding='utf-8') as writeData:\n",
    "    for key, value in uniDict.items():\n",
    "        writeData.write('{}:{}\\n'.format(key, value[0:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('30749476_100bi.txt','w+', encoding='utf-8') as writeData:\n",
    "    for key, value in biGramDict.items():\n",
    "        if len(value) < 100:\n",
    "            writeData.write('{}:{}\\n'.format(key, value))\n",
    "        else:\n",
    "            writeData.write('{}:{}\\n'.format(key, value[0:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('30749476_countVec.txt','w+', encoding='utf-8') as writeData:\n",
    "    for value in countVec:\n",
    "        writeData.write(\",\".join(value))\n",
    "        writeData.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('30749476_vocab.txt','w+', encoding='utf-8') as writeData:\n",
    "    for key,value in Vocab.items():\n",
    "        writeData.write('{}:{}\\n'.format(key, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
