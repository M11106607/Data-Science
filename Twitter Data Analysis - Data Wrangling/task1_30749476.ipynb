{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Parsing Text Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name: Manmeet Singh**<br>\n",
    "**Student Id: 30749476**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**<br>\n",
    "This assessment touches the very first step of analyzing textual data, i.e., extracting data from semi-structured text files. Data-set contains information about COVID-19 related tweets.<br>\n",
    "\n",
    "**Steps to perform:**<br>\n",
    "1. First step of analyzing textual data, i.e., extracting data from semi-structured text files.\n",
    "2. Create XML file with following elements:<br>\n",
    "    a. id: is a 19-digit number.<br>\n",
    "    b. text: is the actual tweet.<br>\n",
    "    c. Created_at: is the date and time that the tweet was created<br>\n",
    "3. Id's must ne unique, filter out non-english tweets.<br>\n",
    "4. Designing efficient regular expressions in order to extract the data from your dataset.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import re\n",
    "import langid\n",
    "import os\n",
    "from langid.langid import LanguageIdentifier, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method used to extract data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps performed to extract the data<br>\n",
    "1. Creating 2 dictionaries, one to store the final data and a temporary dictionary to store Id and text as key value.<br>\n",
    "2. Creating a language identifier, as we load text file in UTF-8 format, file contains emojis as well. So, while clasifying the language there might be an error. Language identifier will identify language even if text contains emojis.<br>\n",
    "3. Creating a list of documents to read.<br>\n",
    "4. Opening each file one by one and performing following steps respectively:<br>\n",
    "    a. Split data first based on regex **(r'\\[(.*)\\]')**. As our data is in format of dictionary in list, it will extract data inside the square braces. It will extract all the data inside the keyword \"data\". It will generate another list, containing tweet data and error logs.<br>\n",
    "    b. Second regex **(r'\\{\"(.*?)\\\"}')** is used to extract the data inside the curly braces based on non-greedy method, as it is possible there maybe some text inside curly braces for \"text\" key. This will create another list. Each list is package of id, text and created_at data.<br>\n",
    "    c. Checking if withheld, copyright, and country_codes exist while looping over the list created, as it is not part of tweets and might casue errors such as string literal or key word error for \"false\". If exist skip and go to next element in list.<br>\n",
    "    d. Check every element of list if it contains all three terms text, id, created_at because error logs does not have text.<br>\n",
    "    e. If given term exist add prefix and suffix of curly braces.<br>\n",
    "    f. Use eval function as it will generate dictioinary from string of dictionary.<br>\n",
    "    g. Extract the tweet from key \"text\".\n",
    "    h. Convert the tweet to UTF-16 format as tweet might contain emojis as UTF-8 cannot comprehend it.<br>\n",
    "    i. Filtering out the non-english tweets.<br>\n",
    "    j. Using regex **(r'[0-9]+-[0-9]+-[0-9]+')** to find the date as it is in UTC format.<br>\n",
    "    k. Adding date as key in NewDict (dictionary). If date already exist then update its value by id:text as key:value.<br>\n",
    "    l. To avoid the duplicate id's check if id exist in value of date key. If exist skip else update to the date value.<br>\n",
    "    m. Steps from a to l will be performed in loop and all the data will be added to NewDict where Date is key and id:text is value.<br>\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating temprory dictionary to store Id as key and text as value\n",
    "temp = {}\n",
    "# initiating New dictionary to store date as key and Id, text as value in dictionary format \n",
    "NewDict = {}\n",
    "# creating a language identifier, as we load text file in UTF-8 format, file contains \n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "\n",
    "# creating a list of all the documents on the given path\n",
    "all_files = os.listdir(\"30749476/\")\n",
    "\n",
    "# creating a for loop to call files one by one, reading them and processing them\n",
    "for texFile in all_files:\n",
    "    path = '30749476/{}'.format(texFile)\n",
    "    \n",
    "    with open(path, encoding = 'utf-8') as file:\n",
    "            data = file.read()\n",
    "    \n",
    "    # Creating frist split of data using the regex\n",
    "    tweetSplit1 = re.split(r'\\[(.*)\\]', data)\n",
    "    # finding all the data as per the given regex\n",
    "    tweetSplit2 = re.split(r'\\{\"(.*?)\\\"}',tweetSplit1[1])\n",
    "    \n",
    "    # reading all the data created by tweetSplit2\n",
    "    for terms in tweetSplit2:\n",
    "        # to avoid string literal and keyword error, a specific format contaning given terms is ignored\n",
    "        if 'withheld' in terms and 'copyright' in terms and 'country_codes' in terms:\n",
    "            continue\n",
    "        if (\"created_at\" in terms) and (\"text\" in terms) and (\"id\" in terms):  # if text contain id, text and created_at terms then process it.\n",
    "            # padding curly braces to create a dictionary format\n",
    "            dictString = '{\"' + terms + '\"}'\n",
    "            # calling eval function to generate dictionary from string of dictionary\n",
    "            content = eval(dictString)\n",
    "            # fetching text from the dictionary\n",
    "            text = content[\"text\"]\n",
    "            # converting text to UTF-16 format to read emojis as well\n",
    "            utf16 = text.encode('utf-16', 'surrogatepass').decode('utf-16')\n",
    "            # if text is english then only process it\n",
    "            if identifier.classify(utf16)[0] == 'en':\n",
    "                # fetching date from the dictionary\n",
    "                toMatch = content[\"created_at\"]\n",
    "                date = re.findall(r'[0-9]+-[0-9]+-[0-9]+',toMatch)[0]\n",
    "                # if date does not exist in dictionary create one\n",
    "                if date not in NewDict:\n",
    "                    NewDict[date] = {}\n",
    "                    # removes duplicate entry of id's\n",
    "                    if content[\"id\"] in NewDict[date]:\n",
    "                        continue\n",
    "                    temp[content[\"id\"]] = content[\"text\"] # creating key value pair of id and text\n",
    "                    NewDict[date].update(temp) # adding to required date\n",
    "                    temp = {}\n",
    "                else:\n",
    "                    if content[\"id\"] in NewDict[date]: # removes duplicate entry of id's\n",
    "                        continue\n",
    "                    temp[content[\"id\"]] = content[\"text\"] # creating key value pair of id and text\n",
    "                    NewDict[date].update(temp) # adding to required date\n",
    "                    temp = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the data to XML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('30749476.xml','w+', encoding='utf-8') as writeData: # write data in xml format\n",
    "    writeData.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    writeData.write('<data>\\n')\n",
    "    \n",
    "    for date, idText in NewDict.items(): \n",
    "        writeData.write('<tweets date=\"{}\">\\n'.format(date))\n",
    "        \n",
    "        for ID, Text in idText.items():\n",
    "            Text = Text.encode('utf-16', 'surrogatepass').decode('utf-16') # decoding the text from UTF-16 again as encoding is UTF-8\n",
    "            writeData.write('<tweet id=\"{}\">{}</tweet>\\n'.format(ID,Text))\n",
    "            \n",
    "        writeData.write('</tweets>\\n')\n",
    "    writeData.write('</data>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading XML file created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing xmltodict library to check if created xml file is laodable or not \n",
    "import xmltodict\n",
    "\n",
    "with open('30749476.xml', 'r', encoding = 'utf-8') as fd:\n",
    "    doc = xmltodict.parse(fd.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
